{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec685ab0-3efc-4fca-94e9-8a69fdf1f2b6",
   "metadata": {},
   "source": [
    "# Simple Tokenizer\n",
    "\n",
    "this code is a cleaned up version of [this colab notebook](https://colab.research.google.com/drive/1y0KnCFZvGVf_odSfcNAws6kcDD7HsI0L?usp=sharing#scrollTo=_paQxu7EOhvg) which andrej karpathy wrote in for his [youtube lesson on tokenizers](https://www.youtube.com/watch?v=zduSFxRajkE). I'll be using it as the tokenizer in all of my test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5001baf5-994b-4d55-a5f5-a27fe25c8c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"안녕하세요 👋 (hello in Korean!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f7f47684-ecb0-478c-acd9-cc4c00d58f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50504, 45397, 54616, 49464, 50836, 32, 128075, 32, 40, 104, 101, 108, 108, 111, 32, 105, 110, 32, 75, 111, 114, 101, 97, 110, 33, 41]\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "# prints python's numeric labels of each character\n",
    "l = [ord(x) for x in \"안녕하세요 👋 (hello in Korean!)\"]\n",
    "print(l)\n",
    "print(len(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3d1c417b-dc55-4f76-9123-9e5e3616b522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[236, 149, 136, 235, 133, 149, 237, 149, 152, 236, 132, 184, 236, 154, 148, 32, 240, 159, 145, 139, 32, 40, 104, 101, 108, 108, 111, 32, 105, 110, 32, 75, 111, 114, 101, 97, 110, 33, 41]\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "# prints the utf-8 numeric labels of each character\n",
    "l = list(\"안녕하세요 👋 (hello in Korean!)\".encode(\"utf-8\"))\n",
    "print(l)\n",
    "\n",
    "# notice that utf-8 is a dynamic encoding scheme that uses 1-4 numbers that range from 0-255. \n",
    "# so a very common character will be a single number from 0-255, and a very rare character will be 4 different numbers 0-255\n",
    "print(len(l))\n",
    "# for purpose of tokenization, it's perfectly reasonable to use utf-8 even though your tokens may end up being\n",
    "# composed of partial characters. it doesn't matter bc everything's still in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "80438c07-ff56-4f17-bd89-53ca3f855407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n",
      "\n",
      " ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] 65\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# the first 200 characters. It's just one continuous text document with all of the works of shakespeare back-to-back\n",
    "print(text[:200])\n",
    "\n",
    "# here are all the unique characters that occur in this text and how many there are\n",
    "chars = sorted(list(set(text)))\n",
    "v = len(chars)\n",
    "print('\\n', chars, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "94b44aae-e141-43d7-97fb-24bdf4eeb124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n",
      "length: 200\n",
      "---\n",
      "[70, 105, 114, 115, 116, 32, 67, 105, 116, 105, 122, 101, 110, 58, 10, 66, 101, 102, 111, 114, 101, 32, 119, 101, 32, 112, 114, 111, 99, 101, 101, 100, 32, 97, 110, 121, 32, 102, 117, 114, 116, 104, 101, 114, 44, 32, 104, 101, 97, 114, 32, 109, 101, 32, 115, 112, 101, 97, 107, 46, 10, 10, 65, 108, 108, 58, 10, 83, 112, 101, 97, 107, 44, 32, 115, 112, 101, 97, 107, 46, 10, 10, 70, 105, 114, 115, 116, 32, 67, 105, 116, 105, 122, 101, 110, 58, 10, 89, 111, 117, 32, 97, 114, 101, 32, 97, 108, 108, 32, 114, 101, 115, 111, 108, 118, 101, 100, 32, 114, 97, 116, 104, 101, 114, 32, 116, 111, 32, 100, 105, 101, 32, 116, 104, 97, 110, 32, 116, 111, 32, 102, 97, 109, 105, 115, 104, 63, 10, 10, 65, 108, 108, 58, 10, 82, 101, 115, 111, 108, 118, 101, 100, 46, 32, 114, 101, 115, 111, 108, 118, 101, 100, 46, 10, 10, 70, 105, 114, 115, 116, 32, 67, 105, 116, 105, 122, 101, 110, 58, 10, 70, 105, 114, 115, 116, 44, 32, 121, 111, 117]\n",
      "length: 200\n"
     ]
    }
   ],
   "source": [
    "tokens = text[:200].encode(\"utf-8\") # raw bytes\n",
    "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience\n",
    "print('---')\n",
    "print(text[:200])\n",
    "print(\"length:\", len(text[:200]))\n",
    "print('---')\n",
    "print(tokens)\n",
    "print(\"length:\", len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5e6992-bb94-4537-8d4e-7a95fdc310ed",
   "metadata": {},
   "source": [
    "We happen to get the same length here because we're using such simple/common characters. If we used foreign characters or emoji then they'd each take up more than one byte. But that won't really be an issue with TinyShakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d1f1e7ef-8507-43d4-a4d7-313a065e4c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\n",
      "length: 533\n",
      "---\n",
      "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 101, 32, 118, 101, 114, 121, 32, 110, 97, 109, 101, 32, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 101, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 101, 32, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 101, 32, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 101, 32, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 101, 32, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 101, 32, 99, 97, 110, 32, 98, 101, 32, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 101, 32, 85, 110, 105, 99, 111, 100, 101, 32, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 101, 32, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 101, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 101, 32, 119, 104, 111, 108, 101, 32, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
      "length: 616\n"
     ]
    }
   ],
   "source": [
    "# just to prove what i mean\n",
    "# text from https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
    "example_text = \"Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\"\n",
    "tokens = example_text.encode(\"utf-8\") # raw bytes\n",
    "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience\n",
    "print('---')\n",
    "print(example_text)\n",
    "print(\"length:\", len(example_text))\n",
    "print('---')\n",
    "print(tokens)\n",
    "print(\"length:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4996c7f2-5078-4c53-ae3a-3c6905168a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, (101, 32)), (5, (58, 10)), (4, (115, 116)), (4, (114, 115)), (4, (114, 101)), (4, (105, 114)), (4, (101, 100)), (4, (101, 97)), (4, (70, 105)), (4, (10, 10)), (3, (122, 101)), (3, (118, 101)), (3, (116, 105)), (3, (116, 104)), (3, (116, 32)), (3, (115, 111)), (3, (112, 101)), (3, (111, 108)), (3, (110, 58)), (3, (108, 118)), (3, (108, 108)), (3, (105, 122)), (3, (105, 116)), (3, (104, 101)), (3, (101, 115)), (3, (101, 110)), (3, (97, 107)), (3, (67, 105)), (3, (46, 10)), (3, (44, 32)), (3, (32, 116)), (3, (32, 114)), (3, (32, 97)), (3, (32, 67)), (3, (10, 70)), (2, (116, 111)), (2, (115, 112)), (2, (114, 32)), (2, (111, 117)), (2, (111, 32)), (2, (108, 58)), (2, (107, 46)), (2, (101, 114)), (2, (100, 46)), (2, (100, 32)), (2, (97, 114)), (2, (97, 110)), (2, (65, 108)), (2, (32, 115)), (2, (32, 102)), (2, (10, 65)), (1, (121, 111)), (1, (121, 32)), (1, (119, 101)), (1, (117, 114)), (1, (117, 32)), (1, (116, 44)), (1, (115, 104)), (1, (114, 116)), (1, (114, 111)), (1, (114, 97)), (1, (114, 44)), (1, (112, 114)), (1, (111, 114)), (1, (111, 99)), (1, (110, 121)), (1, (110, 32)), (1, (109, 105)), (1, (109, 101)), (1, (108, 32)), (1, (107, 44)), (1, (105, 115)), (1, (105, 101)), (1, (104, 97)), (1, (104, 63)), (1, (102, 117)), (1, (102, 111)), (1, (102, 97)), (1, (101, 102)), (1, (101, 101)), (1, (100, 105)), (1, (99, 101)), (1, (97, 116)), (1, (97, 109)), (1, (97, 108)), (1, (89, 111)), (1, (83, 112)), (1, (82, 101)), (1, (66, 101)), (1, (63, 10)), (1, (46, 32)), (1, (32, 121)), (1, (32, 119)), (1, (32, 112)), (1, (32, 109)), (1, (32, 104)), (1, (32, 100)), (1, (10, 89)), (1, (10, 83)), (1, (10, 82)), (1, (10, 66))]\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]): # Pythonic way to iterate consecutive elements\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "# let's only do the first 200 characters for now for demonstration purposes\n",
    "tokens = text[:200].encode(\"utf-8\")\n",
    "\n",
    "stats = get_stats(tokens)\n",
    "\n",
    "#print(stats)\n",
    "print(sorted(((v,k) for k,v in stats.items()), reverse=True))\n",
    "# so these are all the pairs of tokens in the text found in order of how often they show up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a4872443-48a4-4290-9b25-281e5ac0d77c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58, 10)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this was the most common pair\n",
    "top_pair = max(stats, key=stats.get)\n",
    "top_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "552d7eae-4835-4f93-b7a0-5315ef1dd472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 99, 9, 1]\n",
      "[70, 105, 114, 115, 116, 32, 67, 105, 116, 105, 122, 101, 110, 256, 66, 101, 102, 111, 114, 101, 32, 119, 101, 32, 112, 114, 111, 99, 101, 101, 100, 32, 97, 110, 121, 32, 102, 117, 114, 116, 104, 101, 114, 44, 32, 104, 101, 97, 114, 32, 109, 101, 32, 115, 112, 101, 97, 107, 46, 10, 10, 65, 108, 108, 256, 83, 112, 101, 97, 107, 44, 32, 115, 112, 101, 97, 107, 46, 10, 10, 70, 105, 114, 115, 116, 32, 67, 105, 116, 105, 122, 101, 110, 256, 89, 111, 117, 32, 97, 114, 101, 32, 97, 108, 108, 32, 114, 101, 115, 111, 108, 118, 101, 100, 32, 114, 97, 116, 104, 101, 114, 32, 116, 111, 32, 100, 105, 101, 32, 116, 104, 97, 110, 32, 116, 111, 32, 102, 97, 109, 105, 115, 104, 63, 10, 10, 65, 108, 108, 256, 82, 101, 115, 111, 108, 118, 101, 100, 46, 32, 114, 101, 115, 111, 108, 118, 101, 100, 46, 10, 10, 70, 105, 114, 115, 116, 32, 67, 105, 116, 105, 122, 101, 110, 256, 70, 105, 114, 115, 116, 44, 32, 121, 111, 117]\n",
      "length: 195\n"
     ]
    }
   ],
   "source": [
    "# so this function will merge a single pair for us\n",
    "def merge(ids, pair, idx):\n",
    "  # in the list of ints (ids), replace all consecutive occurences of pair with the new token idx\n",
    "  newids = []\n",
    "  i = 0\n",
    "  while i < len(ids):\n",
    "    # if we are not at the very last position AND the pair matches, replace it\n",
    "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "      newids.append(idx)\n",
    "      i += 2\n",
    "    else:\n",
    "      newids.append(ids[i])\n",
    "      i += 1\n",
    "  return newids\n",
    "\n",
    "# an example of how it works\n",
    "print(merge([5, 6, 6, 7, 9, 1], (6, 7), 99))\n",
    "\n",
    "# let's do it with our actual top pair\n",
    "tokens2 = merge(tokens, top_pair, 256) # 256 is the id of our new token\n",
    "print(tokens2)\n",
    "print(\"length:\", len(tokens2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "120da24f-07f3-4298-8e97-9ac546a63a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (58, 10) into a new token 256\n",
      "merging (101, 32) into a new token 257\n",
      "merging (70, 105) into a new token 258\n",
      "merging (258, 114) into a new token 259\n",
      "merging (259, 115) into a new token 260\n",
      "merging (260, 116) into a new token 261\n",
      "merging (101, 100) into a new token 262\n",
      "merging (101, 97) into a new token 263\n",
      "merging (10, 10) into a new token 264\n",
      "merging (261, 32) into a new token 265\n",
      "merging (265, 67) into a new token 266\n",
      "merging (266, 105) into a new token 267\n",
      "merging (267, 116) into a new token 268\n",
      "merging (268, 105) into a new token 269\n",
      "merging (269, 122) into a new token 270\n",
      "merging (270, 101) into a new token 271\n",
      "merging (271, 110) into a new token 272\n",
      "merging (272, 256) into a new token 273\n",
      "merging (116, 104) into a new token 274\n",
      "merging (44, 32) into a new token 275\n",
      "merging (112, 263) into a new token 276\n",
      "merging (276, 107) into a new token 277\n",
      "merging (46, 264) into a new token 278\n",
      "merging (108, 108) into a new token 279\n",
      "merging (32, 114) into a new token 280\n",
      "merging (101, 115) into a new token 281\n",
      "merging (281, 111) into a new token 282\n",
      "merging (282, 108) into a new token 283\n",
      "merging (283, 118) into a new token 284\n",
      "merging (284, 262) into a new token 285\n",
      "merging (114, 257) into a new token 286\n",
      "merging (32, 97) into a new token 287\n",
      "merging (32, 102) into a new token 288\n",
      "merging (274, 101) into a new token 289\n",
      "merging (289, 114) into a new token 290\n",
      "merging (115, 277) into a new token 291\n",
      "merging (291, 278) into a new token 292\n",
      "merging (65, 279) into a new token 293\n",
      "merging (293, 256) into a new token 294\n",
      "merging (111, 117) into a new token 295\n",
      "merging (280, 285) into a new token 296\n",
      "merging (32, 116) into a new token 297\n",
      "merging (297, 111) into a new token 298\n",
      "merging (273, 66) into a new token 299\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 300 # the desired final vocabulary size\n",
    "num_merges = vocab_size - 256\n",
    "ids = list(tokens) # copy so we don't destroy the original list\n",
    "\n",
    "# now let's actually do it\n",
    "merges = {} # (int, int) -> int\n",
    "for i in range(num_merges):\n",
    "  stats = get_stats(ids)\n",
    "  pair = max(stats, key=stats.get)\n",
    "  idx = 256 + i\n",
    "  print(f\"merging {pair} into a new token {idx}\")\n",
    "  ids = merge(ids, pair, idx)\n",
    "  merges[pair] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2596a6b1-f4f8-4051-9012-bc9ed0a08511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 200\n",
      "ids length: 72\n",
      "compression ratio: 2.78X\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens length:\", len(tokens)) # remember tokens are our original tokens\n",
    "print(\"ids length:\", len(ids)) # and ids are new tokens we've made\n",
    "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6402ac6d-c082-45b0-8275-be6d6bc6c477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "�\n"
     ]
    }
   ],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "def decode(ids):\n",
    "  # given ids (list of integers), return Python string\n",
    "  tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "  text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "  return text\n",
    "\n",
    "print(decode([128]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a1e67ea4-1aaa-45ed-88ef-89f723fa2aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 101, 279, 111, 32, 87, 111, 114, 108, 100, 33]\n"
     ]
    }
   ],
   "source": [
    "def encode(text):\n",
    "  # given a string, return list of integers (the tokens)\n",
    "  tokens = list(text.encode(\"utf-8\"))\n",
    "  while len(tokens) >= 2:\n",
    "    stats = get_stats(tokens)\n",
    "    pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "    if pair not in merges:\n",
    "      break # nothing else can be merged\n",
    "    idx = merges[pair]\n",
    "    tokens = merge(tokens, pair, idx)\n",
    "  return tokens\n",
    "\n",
    "print(encode(\"Hello World!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b94abe4b-7a3c-4916-8e84-b6a671afa3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(decode(encode(\"hello world\")))\n",
    "\n",
    "text2 = decode(encode(text))\n",
    "print(text2 == text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc33c2ae-caf2-4c1b-8f7b-091ea4c2c8dd",
   "metadata": {},
   "source": [
    "now instead of just byte-pair encoding, we're also going to enforce what are called \"regex\" rules that basically prevent the tokenizer from combining certain bytes. For example, we humans think that special characters like `!` are notably different from letters, so we don't want the tokenizer to combine `n` and `!` into one token `n!`. It also does other things like merge long sequences of spaces or newline characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "285587d8-14e7-4dd9-9cc5-dccccb4937ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cde6c08a-35bd-445d-b0ac-572b9e2d1bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[220, 220, 220, 23748, 995, 10185]\n",
      "[262, 24748, 1917, 12340]\n"
     ]
    }
   ],
   "source": [
    "# GPT-2 (does not merge spaces)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "print(enc.encode(\"    hello world!!!\")) # 220 is the \" \" token\n",
    "\n",
    "# GPT-4 (merges spaces)\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(enc.encode(\"    hello world!!!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daace11-8797-47c3-a47f-d1ba7c78284c",
   "metadata": {},
   "source": [
    "Personally tho I have no interest in doing all that the way they did given my tiny dataset. I just want a tokenizer with more than 65 characters to give my tiny test model a more realistic modeling experience, and tinyShakespeare doesn't have a whole lot of special characters anyways. So we're gonna enforce an over-simplified version of regex which takes advantage of the fact that the first 13 characters are non-alphabetic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1b5685-ef9d-4867-a929-80764307efd7",
   "metadata": {},
   "source": [
    "# Actually Building It\n",
    "\n",
    "We're gonna make it hella simple, start with our 65 unique characters, and turn them into 128 total tokens that are either composed of letters or non-letters but not both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6484b487-7334-4404-8260-03c8ce907b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n",
      "\n",
      " ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] 65\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# the first 200 characters. It's just one continuous text document with all of the works of shakespeare back-to-back\n",
    "print(text[:200])\n",
    "\n",
    "# here are all the unique characters that occur in this text and how many there are\n",
    "chars = sorted(list(set(text)))\n",
    "v = len(chars)\n",
    "print('\\n', chars, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "231c1fb5-c90b-4bac-9a65-cd6bcfcbef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "char_encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "\n",
    "tokens = char_encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4a32dbf-882c-4e62-8be3-eaf03d5fb5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 128 # the desired final vocabulary size\n",
    "num_merges = vocab_size - v\n",
    "ids = list(tokens) # copy so we don't destroy the original list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8d5c466-ae0c-4723-9922-fdde1233e268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64]\n",
      "['symbol', 'symbol', 'symbol', 'symbol', 'symbol', 'symbol', 'symbol', 'symbol', 'symbol', 'symbol', 'symbol', 'symbol', 'symbol', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter', 'letter']\n"
     ]
    }
   ],
   "source": [
    "base_indices = char_encode(chars)\n",
    "print(base_indices)\n",
    "origin = [ \"symbol\" if i < 13 else \"letter\" for i in base_indices]  # Track token origin\n",
    "print(origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3f909a1-c258-44c1-a102-961f6eca66d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]): # Pythonic way to iterate consecutive elements\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "  # in the list of ints (ids), replace all consecutive occurences of pair with the new token idx\n",
    "  newids = []\n",
    "  i = 0\n",
    "  while i < len(ids):\n",
    "    # if we are not at the very last position AND the pair matches, replace it\n",
    "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "      newids.append(idx)\n",
    "      i += 2\n",
    "    else:\n",
    "      newids.append(ids[i])\n",
    "      i += 1\n",
    "  return newids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27c33cc7-78b9-464b-8089-8c594c672e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (58, 46) into a new token 65\n",
      "merging (6, 1) into a new token 66\n",
      "merging (53, 59) into a new token 67\n",
      "merging (43, 56) into a new token 68\n",
      "merging (47, 52) into a new token 69\n",
      "merging (39, 52) into a new token 70\n",
      "merging (10, 0) into a new token 71\n",
      "merging (65, 43) into a new token 72\n",
      "merging (53, 56) into a new token 73\n",
      "merging (47, 57) into a new token 74\n",
      "merging (0, 0) into a new token 75\n",
      "merging (43, 52) into a new token 76\n",
      "merging (39, 56) into a new token 77\n",
      "merging (39, 58) into a new token 78\n",
      "merging (53, 52) into a new token 79\n",
      "merging (57, 58) into a new token 80\n",
      "merging (50, 50) into a new token 81\n",
      "merging (6, 0) into a new token 82\n",
      "merging (51, 43) into a new token 83\n",
      "merging (58, 53) into a new token 84\n",
      "merging (8, 75) into a new token 85\n",
      "merging (70, 42) into a new token 86\n",
      "merging (46, 43) into a new token 87\n",
      "merging (63, 67) into a new token 88\n",
      "merging (43, 57) into a new token 89\n",
      "merging (52, 53) into a new token 90\n",
      "merging (57, 43) into a new token 91\n",
      "merging (46, 39) into a new token 92\n",
      "merging (56, 43) into a new token 93\n",
      "merging (53, 44) into a new token 94\n",
      "merging (60, 43) into a new token 95\n",
      "merging (47, 58) into a new token 96\n",
      "merging (69, 45) into a new token 97\n",
      "merging (40, 43) into a new token 98\n",
      "merging (50, 43) into a new token 99\n",
      "merging (61, 47) into a new token 100\n",
      "merging (51, 63) into a new token 101\n",
      "merging (46, 47) into a new token 102\n",
      "merging (53, 61) into a new token 103\n",
      "merging (41, 43) into a new token 104\n",
      "merging (44, 73) into a new token 105\n",
      "merging (39, 63) into a new token 106\n",
      "merging (39, 57) into a new token 107\n",
      "merging (41, 46) into a new token 108\n",
      "merging (52, 42) into a new token 109\n",
      "merging (68, 43) into a new token 110\n",
      "merging (50, 42) into a new token 111\n",
      "merging (47, 56) into a new token 112\n",
      "merging (43, 42) into a new token 113\n",
      "merging (59, 58) into a new token 114\n",
      "merging (56, 53) into a new token 115\n",
      "merging (90, 58) into a new token 116\n",
      "merging (50, 47) into a new token 117\n",
      "merging (42, 43) into a new token 118\n",
      "merging (61, 43) into a new token 119\n",
      "merging (46, 78) into a new token 120\n",
      "merging (65, 78) into a new token 121\n",
      "merging (45, 46) into a new token 122\n",
      "merging (13, 109) into a new token 123\n",
      "merging (49, 43) into a new token 124\n",
      "merging (11, 1) into a new token 125\n",
      "merging (53, 53) into a new token 126\n",
      "merging (67, 56) into a new token 127\n"
     ]
    }
   ],
   "source": [
    "# now let's actually do it\n",
    "merges = {} # (int, int) -> int\n",
    "for i in range(num_merges):\n",
    "    #print(i)\n",
    "    stats = get_stats(ids)\n",
    "\n",
    "    # Modified pair selection logic:\n",
    "    while True:\n",
    "        pair = max(stats, key=stats.get)  # Get the most frequent pair initially\n",
    "        #print(pair)\n",
    "\n",
    "        if origin[pair[0]] != origin[pair[1]]: # Check if origins differ\n",
    "            #print(origin[pair[0]],origin[pair[1]],origin[pair[0]] != origin[pair[1]])\n",
    "            del stats[pair]\n",
    "        else:  # If no valid pairs left, break out of the loop\n",
    "            #print(origin[pair[0]],origin[pair[1]],origin[pair[0]] != origin[pair[1]])\n",
    "            break\n",
    "    else:\n",
    "        break  # Valid pair found \n",
    "    \n",
    "    pair = max(stats, key=stats.get)\n",
    "    #print(pair)\n",
    "\n",
    "    idx = v + i\n",
    "    print(f\"merging {pair} into a new token {idx}\")\n",
    "    ids = merge(ids, pair, idx)\n",
    "    merges[pair] = idx\n",
    "    origin.append(origin[pair[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5ffdf00-81f1-4ec0-ab00-a688faa75d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 1115394\n",
      "ids length: 809324\n",
      "compression ratio: 1.38X\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens length:\", len(tokens)) # remember tokens are our original tokens\n",
    "print(\"ids length:\", len(ids)) # and ids are new tokens we've made\n",
    "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fc330c-e67e-4669-8556-8d53c8e843ea",
   "metadata": {},
   "source": [
    "This is pretty ideal because if we did too large of a compression ratio we'd be making our dataset size way too small to be useable. notice how if we hadn't implmenented our symbols vs letters rule we'd end up with a higher compression ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e122fa3-5f80-4c88-872e-b0038d2da7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Ensure the tokenizers directory exists\n",
    "if not os.path.exists('./tokenizers'):\n",
    "    os.makedirs('./tokenizers')\n",
    "\n",
    "# Prepare the tokenizer data to be saved\n",
    "tokenizer_data = {\n",
    "    'stoi': stoi,  # Character to integer mapping\n",
    "    'merges': merges  # Merges dictionary\n",
    "}\n",
    "\n",
    "# Save the tokenizer data using pickle\n",
    "with open('./tokenizers/tokenizer.model', 'wb') as f:\n",
    "    pickle.dump(tokenizer_data, f)\n",
    "\n",
    "print(\"Tokenizer saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14192ed9-ebbf-43a9-9f82-b4b26d89f64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer data using pickle\n",
    "with open('./tokenizers/tokenizer.model', 'rb') as f:\n",
    "    loaded_tokenizer_data = pickle.load(f)\n",
    "\n",
    "# Extract the stoi mapping and merges from the loaded data\n",
    "loaded_stoi = loaded_tokenizer_data['stoi']\n",
    "loaded_merges = loaded_tokenizer_data['merges']\n",
    "\n",
    "print(\"Tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "678784cd-f26e-4a83-922d-3a5f99b2fd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
      "{(58, 46): 65, (6, 1): 66, (53, 59): 67, (43, 56): 68, (47, 52): 69, (39, 52): 70, (10, 0): 71, (65, 43): 72, (53, 56): 73, (47, 57): 74, (0, 0): 75, (43, 52): 76, (39, 56): 77, (39, 58): 78, (53, 52): 79, (57, 58): 80, (50, 50): 81, (6, 0): 82, (51, 43): 83, (58, 53): 84, (8, 75): 85, (70, 42): 86, (46, 43): 87, (63, 67): 88, (43, 57): 89, (52, 53): 90, (57, 43): 91, (46, 39): 92, (56, 43): 93, (53, 44): 94, (60, 43): 95, (47, 58): 96, (69, 45): 97, (40, 43): 98, (50, 43): 99, (61, 47): 100, (51, 63): 101, (46, 47): 102, (53, 61): 103, (41, 43): 104, (44, 73): 105, (39, 63): 106, (39, 57): 107, (41, 46): 108, (52, 42): 109, (68, 43): 110, (50, 42): 111, (47, 56): 112, (43, 42): 113, (59, 58): 114, (56, 53): 115, (90, 58): 116, (50, 47): 117, (42, 43): 118, (61, 43): 119, (46, 78): 120, (65, 78): 121, (45, 46): 122, (13, 109): 123, (49, 43): 124, (11, 1): 125, (53, 53): 126, (67, 56): 127}\n"
     ]
    }
   ],
   "source": [
    "print(stoi)\n",
    "print(merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30b8d8fe-2afe-41f3-ac12-0c53360d0e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [22, 33, 24, 21, 17, 32, 71, 27, 1, 30, 53, 83, 53, 66, 30, 53, 83, 53, 2, 1, 61, 87, 93, 105, 43, 1, 77, 58, 1, 65, 67, 1, 30]\n",
      "Decoded: JULIET:\n",
      "O Romeo, Romeo! wherefore art thou R\n"
     ]
    }
   ],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self, stoi, merges):\n",
    "        self.stoi = stoi\n",
    "        self.merges = merges\n",
    "        self.itos = {i: s for s, i in stoi.items()}  # Inverse mapping for decoding\n",
    "\n",
    "        self.vocab_len = len(stoi) + len(merges)\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Convert the text to a list of token IDs, using space for unknown characters\n",
    "        tokens = [self.stoi.get(c, self.stoi[' ']) for c in text]\n",
    "\n",
    "        # Perform merging with the possibility of nested merges\n",
    "        i = 0\n",
    "        while i < len(tokens) - 1:\n",
    "            pair = (tokens[i], tokens[i + 1])\n",
    "            if pair in self.merges:\n",
    "                # Replace the current pair with its merged token\n",
    "                merged_token = self.merges[pair]\n",
    "                tokens[i] = merged_token\n",
    "                del tokens[i + 1]\n",
    "\n",
    "                # Move back to handle possible nested merges\n",
    "                if i > 0:\n",
    "                    i -= 1\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        def expand_token(token):\n",
    "            # Base case: if the token is a direct mapping, return its character\n",
    "            if token in self.itos:\n",
    "                return self.itos[token]\n",
    "            # Recursive case: if the token is a merged token, expand its constituents\n",
    "            elif token in self.merges.values():\n",
    "                pair = next(key for key, value in self.merges.items() if value == token)\n",
    "                return ''.join(expand_token(t) for t in pair)\n",
    "            # Fallback for unknown tokens\n",
    "            else:\n",
    "                return ''\n",
    "\n",
    "        # Decode each token in the list, handling nested merges recursively\n",
    "        return ''.join(expand_token(token) for token in tokens)\n",
    "\n",
    "# Example usage\n",
    "# Assuming loaded_stoi and loaded_merges are already loaded from the tokenizer.model file\n",
    "\n",
    "tokenizer = SimpleTokenizer(loaded_stoi, loaded_merges)\n",
    "\n",
    "# Encoding text\n",
    "encoded_text = tokenizer.encode(\"JULIET:\\nO Romeo, Romeo! wherefore art thou R\")\n",
    "print(\"Encoded:\", encoded_text)\n",
    "\n",
    "# Decoding back\n",
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "print(\"Decoded:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8eb32900-1bcf-461d-9e4d-82ac3c957629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: '\n",
      "'\n",
      "1: ' '\n",
      "2: '!'\n",
      "3: '$'\n",
      "4: '&'\n",
      "5: '''\n",
      "6: ','\n",
      "7: '-'\n",
      "8: '.'\n",
      "9: '3'\n",
      "10: ':'\n",
      "11: ';'\n",
      "12: '?'\n",
      "13: 'A'\n",
      "14: 'B'\n",
      "15: 'C'\n",
      "16: 'D'\n",
      "17: 'E'\n",
      "18: 'F'\n",
      "19: 'G'\n",
      "20: 'H'\n",
      "21: 'I'\n",
      "22: 'J'\n",
      "23: 'K'\n",
      "24: 'L'\n",
      "25: 'M'\n",
      "26: 'N'\n",
      "27: 'O'\n",
      "28: 'P'\n",
      "29: 'Q'\n",
      "30: 'R'\n",
      "31: 'S'\n",
      "32: 'T'\n",
      "33: 'U'\n",
      "34: 'V'\n",
      "35: 'W'\n",
      "36: 'X'\n",
      "37: 'Y'\n",
      "38: 'Z'\n",
      "39: 'a'\n",
      "40: 'b'\n",
      "41: 'c'\n",
      "42: 'd'\n",
      "43: 'e'\n",
      "44: 'f'\n",
      "45: 'g'\n",
      "46: 'h'\n",
      "47: 'i'\n",
      "48: 'j'\n",
      "49: 'k'\n",
      "50: 'l'\n",
      "51: 'm'\n",
      "52: 'n'\n",
      "53: 'o'\n",
      "54: 'p'\n",
      "55: 'q'\n",
      "56: 'r'\n",
      "57: 's'\n",
      "58: 't'\n",
      "59: 'u'\n",
      "60: 'v'\n",
      "61: 'w'\n",
      "62: 'x'\n",
      "63: 'y'\n",
      "64: 'z'\n",
      "65: 'th'\n",
      "66: ', '\n",
      "67: 'ou'\n",
      "68: 'er'\n",
      "69: 'in'\n",
      "70: 'an'\n",
      "71: ':\n",
      "'\n",
      "72: 'the'\n",
      "73: 'or'\n",
      "74: 'is'\n",
      "75: '\n",
      "\n",
      "'\n",
      "76: 'en'\n",
      "77: 'ar'\n",
      "78: 'at'\n",
      "79: 'on'\n",
      "80: 'st'\n",
      "81: 'll'\n",
      "82: ',\n",
      "'\n",
      "83: 'me'\n",
      "84: 'to'\n",
      "85: '.\n",
      "\n",
      "'\n",
      "86: 'and'\n",
      "87: 'he'\n",
      "88: 'you'\n",
      "89: 'es'\n",
      "90: 'no'\n",
      "91: 'se'\n",
      "92: 'ha'\n",
      "93: 're'\n",
      "94: 'of'\n",
      "95: 've'\n",
      "96: 'it'\n",
      "97: 'ing'\n",
      "98: 'be'\n",
      "99: 'le'\n",
      "100: 'wi'\n",
      "101: 'my'\n",
      "102: 'hi'\n",
      "103: 'ow'\n",
      "104: 'ce'\n",
      "105: 'for'\n",
      "106: 'ay'\n",
      "107: 'as'\n",
      "108: 'ch'\n",
      "109: 'nd'\n",
      "110: 'ere'\n",
      "111: 'ld'\n",
      "112: 'ir'\n",
      "113: 'ed'\n",
      "114: 'ut'\n",
      "115: 'ro'\n",
      "116: 'not'\n",
      "117: 'li'\n",
      "118: 'de'\n",
      "119: 'we'\n",
      "120: 'hat'\n",
      "121: 'that'\n",
      "122: 'gh'\n",
      "123: 'And'\n",
      "124: 'ke'\n",
      "125: '; '\n",
      "126: 'oo'\n",
      "127: 'our'\n"
     ]
    }
   ],
   "source": [
    "for i in range(128):\n",
    "    print(f\"{i}: '{tokenizer.decode([i])}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
