# my virtual environments are rarely properly connected to jupyter so this fixes that
import sys
import os
current_dir = os.getcwd()  # Get the current working directory
venv_dir = os.path.join(current_dir, 'venv') 
python_version = str(sys.version_info.major) + '.' + str(sys.version_info.minor)
site_packages_path = os.path.join(venv_dir, 'lib', 'python' + python_version, 'site-packages')
sys.path.append(site_packages_path) 


# Importing pytorch
import torch
import torch.nn as nn
from torch.nn import functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# imports for the debugging/demonstration setup
import functools
import inspect

# imports for the tokenizer
from tokenizer import SimpleTokenizer, loaded_stoi, loaded_merges

# Imports used for the config
import dataclasses 
from typing import Optional

# Imports used for the model
import re
from typing import Any, List, Sequence, Tuple, Union

# used in the training loop
import time


# this function will be used throughout for debugging/demonstration purposes
# using this is way cleaner than cluttering up our code with print statements
def log_io(func):
    @functools.wraps(func)
    def wrapper(self, *args, **kwargs):
        # Check if logging is enabled globally and for the specific function
        if not self.logging_enabled or func.__name__ in self.disabled_logging_functions:
            return func(self, *args, **kwargs)
        #if not self.logging_enabled:
            #return func(self, *args, **kwargs)

        def log_item(item, name, level=0, is_root=False):
            indent = "    " * level
            if isinstance(item, torch.Tensor):
                print(f"{indent}Tensor '{name}' shape: {item.shape}")
            elif isinstance(item, tuple):
                if is_root and level == 0:
                    # Root level tuple, don't print it as a tuple unless it's a "true" tuple
                    for idx, sub_item in enumerate(item):
                        log_item(sub_item, f"{name}[{idx}]", level)
                else:
                    print(f"{indent}Tuple '{name}':")
                    for idx, sub_item in enumerate(item):
                        log_item(sub_item, f"{name}[{idx}]", level + 1)
            elif isinstance(item, int):
                print(f"{indent}Integer '{name}': Value={item}")
            elif isinstance(item, float):
                print(f"{indent}Float '{name}': Value={item}")
            else:
                print(f"{indent}Other-type '{name}': Type={type(item).__name__}, Value={item}")

        print(f"\n{'='*10}Entering {self.__class__.__name__}.{func.__name__}{'='*10}")
        print("Inputs:")
        arg_names = inspect.getfullargspec(func).args[1:]  # Excluding 'self'
        arg_values = args + tuple(kwargs.values())
        for name, value in zip(arg_names, arg_values):
            log_item(value, name)

        result = func(self, *args, **kwargs)
        print("\nOutputs:")
        if isinstance(result, tuple):
            log_item(result, "output", is_root=True)
        else:
            log_item(result, "output")

        print(f"{'='*10}Exiting {self.__class__.__name__}.{func.__name__}{'='*10}")
        return result
    return wrapper

class LoggingModule(nn.Module):
    def __init__(self):
        super().__init__()
        self.logging_enabled = False
        self.disabled_logging_functions = set()

    def enable_logging(self):
        self.logging_enabled = True

    def disable_logging(self):
        self.logging_enabled = False

    def disable_function_logging(self, func_name):
        self.disabled_logging_functions.add(func_name)

    def enable_function_logging(self, func_name):
        self.disabled_logging_functions.discard(func_name)


device = 'mps'#'cuda' if torch.cuda.is_available() else 'cpu'


# Data transformation and normalization
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

# Downloading and loading the MNIST dataset
train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)
test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)

# Creating data loaders
train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=1000, shuffle=False)


class ReLU(LoggingModule):
    def __init__(self,
                 embed_dim: int,
                 mlp_multiplier: int,
                 output_dim: int,
                 dropout_rate: float = 0.1):
        super().__init__()
        self.embed_dim = embed_dim
        self.mlp_multiplier = mlp_multiplier
        self.hidden_size = embed_dim * mlp_multiplier
        self.dropout_rate = dropout_rate

        # the up and down projections
        self.up_proj = nn.Linear(embed_dim, self.hidden_size)
        self.down_proj = nn.Linear(self.hidden_size, output_dim)
        
    @log_io
    def forward(self, x: torch.Tensor, training: bool = False ) -> torch.Tensor:
        x = x.view(-1, self.embed_dim)
        output = self.down(F.relu(self.up(x)))
        return F.dropout(output, p=self.dropout_rate, training=training)

    @log_io
    def up(self, x):
        return self.up_proj(x)

    @log_io
    def down(self, x):
        return self.down_proj(x)


module = ReLU(28*28, 8, 10)
print(sum(p.numel() for p in module.parameters())/1e3, 'K parameters')
print(module)
module.enable_logging()
output = module(torch.randn(32, 28*28))
del module, output


class GeLU(LoggingModule):
    def __init__(self,
                 embed_dim: int,
                 mlp_multiplier: int,
                 output_dim: int,
                 dropout_rate: float = 0.1):
        super().__init__()
        self.embed_dim = embed_dim
        self.mlp_multiplier = mlp_multiplier
        self.hidden_size = embed_dim * mlp_multiplier
        self.dropout_rate = dropout_rate

        # the up and down projections
        self.up_proj = nn.Linear(embed_dim, self.hidden_size)
        self.down_proj = nn.Linear(self.hidden_size, output_dim)
        
    @log_io
    def forward(self, x: torch.Tensor, training: bool = False ) -> torch.Tensor:
        x = x.view(-1, self.embed_dim)
        output = self.down(F.gelu(self.up(x)))
        return F.dropout(output, p=self.dropout_rate, training=training)

    @log_io
    def up(self, x):
        return self.up_proj(x)

    @log_io
    def down(self, x):
        return self.down_proj(x)


module = GeLU(28*28, 8, 10)
print(sum(p.numel() for p in module.parameters())/1e3, 'K parameters')
print(module)
module.enable_logging()
output = module(torch.randn(32, 28*28))
del module, output


class SiLU(LoggingModule):
    def __init__(self,
                 embed_dim: int,
                 mlp_multiplier: int,
                 output_dim: int,
                 dropout_rate: float = 0.1):
        super().__init__()
        self.embed_dim = embed_dim
        self.mlp_multiplier = mlp_multiplier
        self.hidden_size = embed_dim * mlp_multiplier
        self.dropout_rate = dropout_rate

        # the up and down projections
        self.up_proj = nn.Linear(embed_dim, self.hidden_size)
        self.down_proj = nn.Linear(self.hidden_size, output_dim)
        
    @log_io
    def forward(self, x: torch.Tensor, training: bool = False ) -> torch.Tensor:
        x = x.view(-1, self.embed_dim)
        output = self.down(F.silu(self.up(x)))
        return F.dropout(output, p=self.dropout_rate, training=training)

    @log_io
    def up(self, x):
        return self.up_proj(x)

    @log_io
    def down(self, x):
        return self.down_proj(x)


module = SiLU(28*28, 8, 10)
print(sum(p.numel() for p in module.parameters())/1e3, 'K parameters')
print(module)
module.enable_logging()
output = module(torch.randn(32, 28*28))
del module, output


class ReGLU(LoggingModule):
    def __init__(self,
                 embed_dim: int,
                 mlp_multiplier: int,
                 output_dim: int,
                 dropout_rate: float = 0.1):
        super().__init__()
        self.embed_dim = embed_dim
        self.mlp_multiplier = mlp_multiplier
        self.hidden_size = embed_dim * mlp_multiplier
        self.dropout_rate = dropout_rate

        # the gate, up and down projections
        self.gate_proj = nn.Linear(embed_dim, self.hidden_size)
        self.up_proj = nn.Linear(embed_dim, self.hidden_size)
        self.down_proj = nn.Linear(self.hidden_size, output_dim)
        
    @log_io
    def forward(self, x: torch.Tensor, training: bool = False ) -> torch.Tensor:
        x = x.view(-1, self.embed_dim)
        output = self.down(self.fuse(self.gate(x),self.up(x)))
        return F.dropout(output, p=self.dropout_rate, training=training)

    @log_io
    def up(self, x):
        return self.up_proj(x)

    @log_io
    def gate(self, x):
        return F.relu(self.gate_proj(x))

    @log_io
    def fuse(self, up, gate):
        return up * gate

    @log_io
    def down(self, x):
        return self.down_proj(x)


module = ReGLU(28*28, 4, 10)
print(sum(p.numel() for p in module.parameters())/1e3, 'K parameters')
print(module)
module.enable_logging()
output = module(torch.randn(32, 28*28))
del module, output


class GeGLU(LoggingModule):
    def __init__(self,
                 embed_dim: int,
                 mlp_multiplier: int,
                 output_dim: int,
                 dropout_rate: float = 0.1):
        super().__init__()
        self.embed_dim = embed_dim
        self.mlp_multiplier = mlp_multiplier
        self.hidden_size = embed_dim * mlp_multiplier
        self.dropout_rate = dropout_rate

        # the gate, up and down projections
        self.gate_proj = nn.Linear(embed_dim, self.hidden_size)
        self.up_proj = nn.Linear(embed_dim, self.hidden_size)
        self.down_proj = nn.Linear(self.hidden_size, output_dim)
        
    @log_io
    def forward(self, x: torch.Tensor, training: bool = False ) -> torch.Tensor:
        x = x.view(-1, self.embed_dim)
        output = self.down(self.fuse(self.gate(x),self.up(x)))
        return F.dropout(output, p=self.dropout_rate, training=training)

    @log_io
    def up(self, x):
        return self.up_proj(x)

    @log_io
    def gate(self, x):
        return F.gelu(self.gate_proj(x))

    @log_io
    def fuse(self, up, gate):
        return up * gate

    @log_io
    def down(self, x):
        return self.down_proj(x)


module = GeGLU(28*28, 4, 10)
print(sum(p.numel() for p in module.parameters())/1e3, 'K parameters')
print(module)
module.enable_logging()
output = module(torch.randn(32, 28*28))
del module, output


class SwiGLU(LoggingModule):
    def __init__(self,
                 embed_dim: int,
                 mlp_multiplier: int,
                 output_dim: int,
                 dropout_rate: float = 0.1):
        super().__init__()
        self.embed_dim = embed_dim
        self.mlp_multiplier = mlp_multiplier
        self.hidden_size = embed_dim * mlp_multiplier
        self.dropout_rate = dropout_rate

        # the gate, up and down projections
        self.gate_proj = nn.Linear(embed_dim, self.hidden_size)
        self.up_proj = nn.Linear(embed_dim, self.hidden_size)
        self.down_proj = nn.Linear(self.hidden_size, output_dim)
        
    @log_io
    def forward(self, x: torch.Tensor, training: bool = False ) -> torch.Tensor:
        x = x.view(-1, self.embed_dim)
        output = self.down(self.fuse(self.gate(x),self.up(x)))
        return F.dropout(output, p=self.dropout_rate, training=training)

    @log_io
    def up(self, x):
        return self.up_proj(x)

    @log_io
    def gate(self, x):
        return F.silu(self.gate_proj(x))

    @log_io
    def fuse(self, up, gate):
        return up * gate

    @log_io
    def down(self, x):
        return self.down_proj(x)


module = SwiGLU(28*28, 4, 10)
print(sum(p.numel() for p in module.parameters())/1e3, 'K parameters')
print(module)
module.enable_logging()
output = module(torch.randn(32, 28*28))
del module, output


class ReSplit(LoggingModule):
    def __init__(self,
                 embed_dim: int,
                 mlp_multiplier: int,
                 output_dim: int,
                 dropout_rate: float = 0.1):
        super().__init__()
        self.embed_dim = embed_dim
        self.mlp_multiplier = mlp_multiplier
        self.hidden_size = embed_dim * mlp_multiplier
        self.dropout_rate = dropout_rate

        self.gate_proj = nn.Linear(embed_dim, self.hidden_size//2)
        self.up_proj = nn.Linear(embed_dim, self.hidden_size//2)
        self.down_proj = nn.Linear(self.hidden_size, output_dim)

    @log_io
    def forward(self, x, training=False):
        x = x.view(-1, self.embed_dim)
        up = self.up(x)
        pos, neg = self.gate(x)
        fuse = self.fuse(up, pos, neg)
        output = self.down(fuse)
        return F.dropout(output, p=self.dropout_rate, training=training)

    @log_io
    def up(self, x):
        return self.up_proj(x)

    @log_io
    def gate(self, x):
        gate = self.gate_proj(x)
        pos = F.relu(gate)
        neg = F.relu(-1.0*gate)
        return pos, neg

    @log_io
    def fuse(self, up, pos, neg):
        pos_fuse = up * pos
        neg_fuse = up * neg
        return torch.cat([pos_fuse, neg_fuse], dim=-1)

    @log_io
    def down(self, x):
        return self.down_proj(x)


module = ReSplit(28*28, 8, 10)
print(sum(p.numel() for p in module.parameters())/1e3, 'K parameters')
print(module)
module.enable_logging()
output = module(torch.randn(32, 28*28))
del module, output














import matplotlib.pyplot as plt

# Function to train and test the model
def train_and_test(model, runs=1, epochs=1):
    accuracies = []
    for run in range(runs):
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        
        for epoch in range(1, epochs + 1):
            model.train()
            for data, target in train_loader:
                data, target = data.to(device), target.to(device)
                optimizer.zero_grad()
                output = model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()

        model.eval()
        correct = 0
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()

        accuracy = 100. * correct / len(test_loader.dataset)
        accuracies.append(accuracy)
        print(f"Run {run + 1}/{runs}, Accuracy: {accuracy:.2f}%")
    
    return sum(accuracies) / len(accuracies)

# Testing different MLPs
mlp_variants = [ReGLU(28*28, 4, 10).to(device), GeGLU(28*28, 4, 10).to(device), SplitGLU(28*28, 8, 10).to(device)]
results = {}

for mlp_variant in mlp_variants:
    average_accuracy = train_and_test(mlp_variant)
    results[mlp_variant.__class__.__name__] = average_accuracy

# Plotting the results
plt.bar(results.keys(), results.values())
plt.xlabel('MLP Variants')
plt.ylabel('Average Test Accuracy (%)')
plt.title('Comparison of MLP Variants on MNIST')
plt.ylim(95, 100)  # Set the y-axis to start at 95% and end at 100%
plt.show()




